{
    "name": "attention",
    "attention_activation": "elu",
    "heads_num": 8,
    "attention_layers": 2
}